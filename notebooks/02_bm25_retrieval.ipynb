{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 retrieval\n",
    "\n",
    "Self-contained implementation and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e131334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ../data/annotations_dataset_new.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATASET_PATH = Path('..') / 'data' / 'annotations_dataset_full.json'\n",
    "if not DATASET_PATH.exists():\n",
    "    DATASET_PATH = Path('..') / 'data' / 'annotations_dataset_new.json'\n",
    "print('Dataset:', DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fa940",
   "metadata": {},
   "source": [
    "## Dataset stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a458824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Songs: 3291\n",
      "Annotations: 22220\n",
      "Avg annotations per song: 6.75\n"
     ]
    }
   ],
   "source": [
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "songs = len(data)\n",
    "annotations = sum(len(s.get('annotations', [])) for s in data)\n",
    "print('Songs:', songs)\n",
    "print('Annotations:', annotations)\n",
    "print('Avg annotations per song:', round(annotations / songs, 2) if songs else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a592d",
   "metadata": {},
   "source": [
    "## Load pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1721b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs: 22220\n",
      "Using subset: 2000\n"
     ]
    }
   ],
   "source": [
    "fragments = []\n",
    "annotations = []\n",
    "metadata = []\n",
    "\n",
    "for song in data:\n",
    "    for ann in song.get('annotations', []):\n",
    "        fragments.append(ann.get('fragment', ''))\n",
    "        annotations.append(ann.get('annotation', ''))\n",
    "        metadata.append({\n",
    "            'artist': song.get('artist', ''),\n",
    "            'title': song.get('title', ''),\n",
    "            'votes': ann.get('votes', 0),\n",
    "        })\n",
    "\n",
    "print('Pairs:', len(fragments))\n",
    "\n",
    "MAX_EXAMPLES = 2000  # set None for full run\n",
    "if MAX_EXAMPLES:\n",
    "    import random\n",
    "    idx = list(range(len(fragments)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(idx)\n",
    "    idx = idx[:MAX_EXAMPLES]\n",
    "    fragments = [fragments[i] for i in idx]\n",
    "    annotations = [annotations[i] for i in idx]\n",
    "    metadata = [metadata[i] for i in idx]\n",
    "    print('Using subset:', len(fragments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35313d8b",
   "metadata": {},
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f22f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'[A-Za-z0-9\\u0400-\\u04FF]+', text.lower())\n",
    "\n",
    "class BM25Retriever:\n",
    "    def __init__(self, fragments, annotations, metadata, k1=1.5, b=0.75):\n",
    "        self.fragments = fragments\n",
    "        self.annotations = annotations\n",
    "        self.metadata = metadata\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "        self.doc_tokens = [tokenize(t) for t in self.fragments]\n",
    "        self.doc_lens = np.array([len(t) for t in self.doc_tokens], dtype=np.float32)\n",
    "        self.avgdl = float(np.mean(self.doc_lens)) if self.doc_lens.size else 0.0\n",
    "        self.N = len(self.doc_tokens)\n",
    "\n",
    "        self.term_freqs = [Counter(t) for t in self.doc_tokens]\n",
    "        self.doc_freqs = {}\n",
    "        for tf in self.term_freqs:\n",
    "            for term in tf.keys():\n",
    "                self.doc_freqs[term] = self.doc_freqs.get(term, 0) + 1\n",
    "\n",
    "        self.idf = {\n",
    "            term: math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)\n",
    "            for term, df in self.doc_freqs.items()\n",
    "        }\n",
    "\n",
    "    def _score(self, query_tokens):\n",
    "        scores = np.zeros(self.N, dtype=np.float32)\n",
    "        if self.N == 0:\n",
    "            return scores\n",
    "\n",
    "        for i, tf in enumerate(self.term_freqs):\n",
    "            dl = self.doc_lens[i]\n",
    "            denom_base = self.k1 * (1.0 - self.b + self.b * (dl / self.avgdl)) if self.avgdl > 0 else 0.0\n",
    "            score = 0.0\n",
    "            for term in query_tokens:\n",
    "                if term not in self.idf:\n",
    "                    continue\n",
    "                freq = tf.get(term, 0)\n",
    "                if freq == 0:\n",
    "                    continue\n",
    "                score += self.idf[term] * (freq * (self.k1 + 1.0)) / (freq + denom_base)\n",
    "            scores[i] = score\n",
    "        return scores\n",
    "\n",
    "    def find_similar(self, query, top_k=3):\n",
    "        scores = self._score(tokenize(query))\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        return [\n",
    "            {\n",
    "                'fragment': self.fragments[idx],\n",
    "                'annotation': self.annotations[idx],\n",
    "                'similarity': float(scores[idx]),\n",
    "                'artist': self.metadata[idx]['artist'],\n",
    "                'title': self.metadata[idx]['title'],\n",
    "                'votes': self.metadata[idx]['votes'],\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "def evaluate_bm25(fragments, annotations, metadata):\n",
    "    retriever = BM25Retriever(fragments, annotations, metadata)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    predictions = []\n",
    "    references = []\n",
    "    similarities = []\n",
    "\n",
    "    for i, fragment in enumerate(fragments):\n",
    "        scores = retriever._score(tokenize(fragment))\n",
    "        scores[i] = -1e9\n",
    "        top_indices = np.argsort(scores)[-3:][::-1]\n",
    "\n",
    "        predicted = annotations[top_indices[0]]\n",
    "        true_annotation = annotations[i]\n",
    "\n",
    "        predictions.append(predicted)\n",
    "        references.append(true_annotation)\n",
    "        similarities.append(scores[top_indices[0]])\n",
    "\n",
    "        if predicted == true_annotation:\n",
    "            correct_top1 += 1\n",
    "        if true_annotation in [annotations[idx] for idx in top_indices]:\n",
    "            correct_top3 += 1\n",
    "\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    bleu = corpus_bleu(predictions, [[r] for r in references])\n",
    "\n",
    "    return {\n",
    "        'method': 'BM25',\n",
    "        'top1_accuracy': correct_top1 / len(fragments) if fragments else 0.0,\n",
    "        'top3_accuracy': correct_top3 / len(fragments) if fragments else 0.0,\n",
    "        'avg_similarity': float(np.mean(similarities)) if similarities else 0.0,\n",
    "        'rouge1': float(np.mean(rouge_scores['rouge1'])) if rouge_scores['rouge1'] else 0.0,\n",
    "        'rouge2': float(np.mean(rouge_scores['rouge2'])) if rouge_scores['rouge2'] else 0.0,\n",
    "        'rougeL': float(np.mean(rouge_scores['rougeL'])) if rouge_scores['rougeL'] else 0.0,\n",
    "        'bleu': bleu.score,\n",
    "        'total_examples': len(fragments),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee70218",
   "metadata": {},
   "source": [
    "## Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'BM25 Retrieval',\n",
       " 'top1_accuracy': None,\n",
       " 'top3_accuracy': None,\n",
       " 'avg_similarity': None,\n",
       " 'rouge1': 0.014,\n",
       " 'rouge2': 0.0019,\n",
       " 'rougeL': 0.0124,\n",
       " 'bleu': None,\n",
       " 'total_examples': 2000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_results = evaluate_bm25(fragments, annotations, metadata)\n",
    "bm25_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c350878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/bm25_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_path = Path('..') / 'data' / 'bm25_results.json'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(bm25_results, f, ensure_ascii=False, indent=2)\n",
    "print('Saved:', out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0783c9d",
   "metadata": {},
   "source": [
    "## Demo query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d91239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fragment': 'Город под подошвой\\n Город под подошвой — этот город под подошвой',\n",
       "  'annotation': '«Город под подошвой» — песня российского рэпера Оксимирона (Oxxxymiron).',\n",
       "  'similarity': 27.536102294921875,\n",
       "  'artist': 'CMH',\n",
       "  'title': 'GAZZ',\n",
       "  'votes': 3},\n",
       " {'fragment': 'Город «А», город «Z»',\n",
       "  'annotation': 'Буквы, которыми обозначали два крупнейших города Казахстана на номерных знаках автомобилей:\\n\\nА – Алматы\\nZ – Астана\\n\\nqurt коренной Астанчанин, но в данный момент проживает и развивается в южной столице – Алматы.',\n",
       "  'similarity': 9.259902000427246,\n",
       "  'artist': '104',\n",
       "  'title': 'КОПЕР (COPER)',\n",
       "  'votes': 4}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = BM25Retriever(fragments, annotations, metadata)\n",
    "retriever.find_similar('Я вижу город под подошвой', top_k=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
